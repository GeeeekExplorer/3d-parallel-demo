# parallel-demo
使用`torch.distributed`包实现DP/TP/PP

命令`torchrun --nproc_per_node=2 (dp|tp|pp).py`即可运行
